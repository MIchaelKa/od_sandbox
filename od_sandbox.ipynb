{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc4ac0-965e-4f34-971b-40bfb40dbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6a638-0fdc-4759-aef6-d458b04637f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5cac7-e398-4fc3-910c-103898c84291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.utils import seed_everything\n",
    "from train import get_dataset\n",
    "from draw_utils import visualize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42774cfd-2e48-4b98-92bc-37cd9e6bdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.logger import logger\n",
    "import logging\n",
    "\n",
    "# DEBUG INFO WARNING ERROR CRITICAL\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c7a20-38f6-46ea-bb49-c00403bd6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe310d0-c054-40e3-98ef-aa867e257aa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be67ab1-3411-4c6c-8d8d-6c5d5e7cf18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bcfe3-5653-469d-b18d-956d962d78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = resnet_fpn_backbone(\n",
    "    'resnet50', # resnet18, resnet50\n",
    "    pretrained=True,\n",
    "    trainable_layers=5, # all layers\n",
    "    # trainable_layers=3,\n",
    "    returned_layers=[2,3,4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809665f-7a62-414d-9537-10180b2e657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf5a2a-9b2f-4d0b-a293-e7dd4db52ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(1,3,384,384)\n",
    "feature_dict = backbone(test_tensor)\n",
    "feature_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a0159-635d-4517-8f4f-d1446828a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(feature_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3a9de-da99-4cf7-98c7-febf4345e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in features:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbeee46-1dc9-4bf9-ae38-f6bec133d014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83fe9786-3b73-4aa9-90bd-e73edbc1f4dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PennFudanPed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b518291-af42-4e09-aed3-306a3e405114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.penn_fudan_dataset import PennFudanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b10505-eba2-4f74-9972-344fc2b6981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PENN_FUDAT_ROOT_PATH = './data/PennFudanPed'\n",
    "dataset_train = PennFudanDataset(\n",
    "    PENN_FUDAT_ROOT_PATH,\n",
    "    train=True,\n",
    "    stride=8,\n",
    "    format='xyxy' # xyxy, cxcywh\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef98d28-26a4-443b-8c88-5ced33f7780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask, bboxs = dataset_train[0]\n",
    "img.shape, mask.shape, bboxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d8abb-fd64-4db6-aae6-5314c41537d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(dataset_train, count=5, size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd23fd0-e734-4492-b38e-fb981ce083b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pascal VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4d00d-a054-4258-919c-7ff006993f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.pascal_voc_dataset import PascalVOCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bb496-3028-4de7-8e8c-960034d6c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_voc = PascalVOCDataset('./data/VOCdevkit/', 'TRAIN', transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081d746-3639-49ee-a53f-9503fdd5c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a872a4-ec0a-4963-ac43-bb437e025ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(dataset_voc, count=5, size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b118fab-231e-4c8b-8a13-009d9a57d2ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b629ee-11e8-43a8-b5f0-5824a99a2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from centernet import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b8d73-377a-47a8-8f0c-b52038ad5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0c795-bd85-4b9e-9653-b0c1367c8052",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask, bboxs = dataset_train[0]\n",
    "img.shape, mask.shape, bboxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f298c-1768-441b-ab6b-87c6a4899f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790fb97-a0b7-431d-8fe6-7a9fae587855",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97001dbf-5903-4b8a-8166-6b836394f5ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c523a9-1e6c-4b38-9358-39065ccde226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_utils import make_prediction\n",
    "from centernet import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40a4f0-8e3b-4c89-929b-a2bf12d92587",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'penn_fud' # voc, penn_fud\n",
    "dataset_train, dataset_test = get_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b58b14-da61-4ef4-80b4-a5d25b7a6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model_name = 'centernet_v2'\n",
    "model_save_name = f'./ckpts/{model_name}.pth'\n",
    "model.load_state_dict(torch.load(model_save_name))\n",
    "logger.info(f'Model loaded from {model_save_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5d3fc-6fd4-484f-95e4-b2554e29da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(model, dataset_test, index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ae258-039e-418a-994f-ec4108390a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(model, dataset_train, index=0, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca1216-76ea-4793-b296-2e4b551ae9e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87fae9-d67e-40b1-ab1e-41f1b46bf103",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_kernel = np.array([\n",
    "    [0.0625, 0.125, 0.0625],\n",
    "    [0.125, 0.25, 0.125],\n",
    "    [0.0625, 0.125, 0.0625]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625649c-b869-44b2-9d0f-ec41b8e68d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_kernel_size = g_kernel.shape[0]\n",
    "w = g_kernel_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37f92c-105c-4fb4-9b98-23e2a551f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = 1\n",
    "\n",
    "center_mask = np.zeros((40, 40), dtype='float32')\n",
    "center_mask[y-w:y+w+1,x-w:x+w+1] = g_kernel\n",
    "show_images([center_mask], [\"mask\"], size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d2e00-798f-4d2a-b58c-81c9115cd12a",
   "metadata": {},
   "source": [
    "# FCOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a75d3-7e69-4b73-ae5d-a932fbedec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720ce71-ccd7-4f56-9b70-11c21ffa7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = resnet_fpn_backbone(\n",
    "    'resnet18', # resnet18, resnet50\n",
    "    pretrained=True,\n",
    "    trainable_layers=5, # all layers\n",
    "    # trainable_layers=3,\n",
    "    returned_layers=[2,3,4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35708f8b-2c52-416d-a70f-136554aacfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = ((8,), (16,), (32,), (64,))  # equal to strides of multi-level feature map\n",
    "aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one anchor\n",
    "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ea711-8c38-48a1-9b58-bb6b9189ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = 300\n",
    "max_size = 1333\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4fb2b-3a59-4997-88ea-4e1414845838",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f0fc8-c787-414f-9df4-19d37326def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.rand(1, 3, *image_size)\n",
    "\n",
    "bboxs = torch.tensor([\n",
    "    [5,10,20,30],\n",
    "    [50,50,150,300],\n",
    "    [200,200,220,250]\n",
    "])\n",
    "# bboxsbboxs.unsqueeze(0)\n",
    "\n",
    "labels = torch.tensor([1,2,3]).long()\n",
    "targets = dict(\n",
    "    boxes=bboxs,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7278b78-4f4b-4292-ad7f-52f5bf587fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = transform(images, [targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc3e8a-c0a4-4d03-84ed-1a09b42e6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = images.tensors.shape[-2:]\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b718b-c937-40ea-a0b6-049c12468cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = backbone(images.tensors)\n",
    "features = list(features.values())\n",
    "for f in features:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf713b-7e52-4186-a22f-94bab1134030",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = anchor_generator(images, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84174f-79de-4e13-8ea7-e57e02cf8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_per_image = anchors[0]\n",
    "targets_per_image = targets[0]\n",
    "len(anchors_per_image), targets_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3afd91-106e-46c6-8529-52991d435fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_num = [f.shape[-2:].numel() for f in features]\n",
    "num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
    "\n",
    "anchor_idx = np.cumsum([0] + num_anchors_per_level)\n",
    "anchor_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6928b18-14cc-4830-80a1-98c793d679ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCOS.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa23a93-7397-4403-b88c-b6212766a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_sampling_radius = 1.5\n",
    "\n",
    "gt_boxes = targets_per_image[\"boxes\"]\n",
    "gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # Nx2\n",
    "anchor_centers = (anchors_per_image[:, :2] + anchors_per_image[:, 2:]) / 2  # N\n",
    "anchor_sizes = anchors_per_image[:, 2] - anchors_per_image[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8aa76a-888d-474c-b430-b7008089567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_match = (anchor_centers[:, None, :] - gt_centers[None, :, :]).abs_().max(\n",
    "    dim=2\n",
    ").values < center_sampling_radius * anchor_sizes[:, None]\n",
    "pairwise_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ea25f-10a3-4ecb-a2d4-e7d7804d283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise distance between N points and M boxes\n",
    "x, y = anchor_centers.unsqueeze(dim=2).unbind(dim=1)  # (N, 1)\n",
    "x0, y0, x1, y1 = gt_boxes.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n",
    "pairwise_dist = torch.stack([x - x0, y - y0, x1 - x, y1 - y], dim=2)  # (N, M)\n",
    "pairwise_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19362af0-a8ae-4182-8a2f-4b5446ad5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor point must be inside gt\n",
    "pairwise_match &= pairwise_dist.min(dim=2).values > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509d2ae-7bef-4d39-95eb-43751bc82346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each anchor is only responsible for certain scale range.\n",
    "lower_bound = anchor_sizes * 4\n",
    "lower_bound[: num_anchors_per_level[0]] = 0\n",
    "upper_bound = anchor_sizes * 8\n",
    "upper_bound[-num_anchors_per_level[-1] :] = float(\"inf\")\n",
    "\n",
    "pairwise_dist = pairwise_dist.max(dim=2).values\n",
    "pairwise_match &= (pairwise_dist > lower_bound[:, None]) & (pairwise_dist < upper_bound[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a79d7-1415-4de8-9b49-e7c44b6410bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the GT box with minimum area, if there are multiple GT matches\n",
    "gt_areas = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])  # N\n",
    "pairwise_match = pairwise_match.to(torch.float32) * (1e8 - gt_areas[None, :])\n",
    "min_values, matched_idx = pairwise_match.max(dim=1)  # R, per-anchor match\n",
    "matched_idx[min_values < 1e-5] = -1  # unmatched anchors are assigned -1\n",
    "\n",
    "matched_idxs_per_image = matched_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e32f97-6c4a-49e1-98c3-958ab6e3181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCOSHead.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240a40d-3b09-42b7-aac6-3fb613bf4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_classes_targets = targets_per_image[\"labels\"][matched_idxs_per_image.clip(min=0)]\n",
    "gt_boxes_targets = targets_per_image[\"boxes\"][matched_idxs_per_image.clip(min=0)]\n",
    "\n",
    "gt_classes_targets[matched_idxs_per_image < 0] = -1  # background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38980bf5-12b8-49fd-a760-45e8ba06a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foregroud_mask = gt_classes_targets >= 0\n",
    "foregroud_mask = matched_idxs_per_image != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67eb54-7538-4cbd-a585-d56cc497a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_boxes_targets[foregroud_mask]\n",
    "# anchors_per_image[foregroud_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fb6f9-87ed-4713-a7cd-faee1817e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_draw = np.ones((*image_size, 3)) * 255\n",
    "\n",
    "fm_idx = 3\n",
    "# anchors_to_show = anchors_per_image[anchor_idx[fm_idx]:anchor_idx[fm_idx+1]]\n",
    "anchors_to_show = anchors_per_image[foregroud_mask]\n",
    "\n",
    "for bbox in anchors_to_show:\n",
    "    bbox = np.int32(bbox)\n",
    "    # print(bbox)\n",
    "    cv2.rectangle(image_to_draw, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (255, 0, 0), 1)\n",
    "\n",
    "for bbox in targets_per_image['boxes']:\n",
    "    bbox = np.int32(bbox)\n",
    "    # print(bbox)\n",
    "    cv2.rectangle(image_to_draw, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (0, 255, 0), 1)\n",
    "    \n",
    "plt.imshow(image_to_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64474dce-2ca6-4048-ae62-9ddbc35634ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_draw = np.ones((*image_size, 3)) * 255\n",
    "\n",
    "fm_idx = 3\n",
    "# anchors_to_show = anchors_per_image[anchor_idx[fm_idx]:anchor_idx[fm_idx+1]]\n",
    "anchors_to_show = anchors_per_image[foregroud_mask]\n",
    "\n",
    "for bbox in anchors_to_show:\n",
    "    bbox = np.int32(bbox)\n",
    "    # print(bbox)\n",
    "    cv2.rectangle(image_to_draw, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (255, 0, 0), 1)\n",
    "\n",
    "for bbox in targets_per_image['boxes']:\n",
    "    bbox = np.int32(bbox)\n",
    "    # print(bbox)\n",
    "    cv2.rectangle(image_to_draw, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (0, 255, 0), 1)\n",
    "    \n",
    "plt.imshow(image_to_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a226bdf-9ea1-4c75-aedd-171d0be7c317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
